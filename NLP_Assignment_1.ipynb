{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assignment-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+XAXFlrnNTqx0J77Icagn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R5F_t57Zx3T",
        "outputId": "3c69cb8e-661b-4b5e-e186-29ed6575f570"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QNO 3. Find stem and lemma words for the given words?\n",
        "\n",
        "“cats\"  \n",
        "\"trouble\"  \n",
        "\"troubling\"  \n",
        "\"troubled\"  \n",
        "“having”  \n",
        "“Corriendo”  \n",
        "“at”  \n",
        "“was\""
      ],
      "metadata": {
        "id": "ccA2fy8uoenC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "lm = WordNetLemmatizer()\n",
        "words = [\"cats\", \"trouble\", \"troubling\", \"troubled\", \"having\", \"Corriendo\", \"at\", \"was\"]\n",
        "\n",
        "print(\"Stem Words : \")\n",
        "for w in words:\n",
        "    print(w, \" = \", ps.stem(w))\n",
        "\n",
        "print(\"\\nLemma Words : \")\n",
        "for w in words:\n",
        "    print(w, \" = \", lm.lemmatize(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7PVIB5Jo7se",
        "outputId": "3247b556-f5a3-446d-d58a-4e46c7980173"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stem Words : \n",
            "cats  =  cat\n",
            "trouble  =  troubl\n",
            "troubling  =  troubl\n",
            "troubled  =  troubl\n",
            "having  =  have\n",
            "Corriendo  =  corriendo\n",
            "at  =  at\n",
            "was  =  wa\n",
            "\n",
            "Lemma Words : \n",
            "cats  =  cat\n",
            "trouble  =  trouble\n",
            "troubling  =  troubling\n",
            "troubled  =  troubled\n",
            "having  =  having\n",
            "Corriendo  =  Corriendo\n",
            "at  =  at\n",
            "was  =  wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QNO 4. Find stop words and BOW from the given paragraph?\n",
        "\n",
        "*“The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.”*"
      ],
      "metadata": {
        "id": "zgWUKUpFbQlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "RlB-YsRRY5HE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae56312-74cd-4967-f8ea-3eaee9ac91e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop Words are : \n",
            " ['is', 'of', 'the', 'and', 'most', 'for', 'and', 'you', 'can', 'the', 'of', 'in', 'the', 'from', 'a', 'you', 'can', 'your', 'into', 'and', 'then', 'the', 'if', 'it', 'in', 'the', 'of', 'by']\n",
            "\n",
            "Bag Of Words and Vectors for all Sentences are : \n",
            "['nltk', 'library', 'one', 'oldest', 'commonly', 'used', 'python', 'libraries', 'natural', 'language', 'processing', '.', 'supports', 'stop', 'word', 'removal', ',', 'find', 'list', 'words', 'corpus', 'module', 'to', 'remove', 'sentence', 'divide', 'text', 'exits', 'provided']\n",
            "SENTENCE- 1 : The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing.\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "SENTENCE- 2 : NLTK supports stop word removal, and you can find the list of stop words in the corpus module.\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "SENTENCE- 3 : To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 3, 0, 0, 1, 2, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "paragraph = \"\"\"The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. \n",
        "NLTK supports stop word removal, and you can find the list of stop words in the corpus module. \n",
        "To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\"\"\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(paragraph)\n",
        "sentence = sent_tokenize(paragraph)\n",
        "\n",
        "stopwords = [w for w in tokens if w in stop_words]\n",
        "print(\"Stop Words are : \\n\",stopwords)\n",
        "\n",
        "seen = set()\n",
        "vocab = [x.lower() for x in tokens if not (x in seen or seen.add(x))]\n",
        "filtered_vocab = [w.lower() for w in vocab if w not in stopwords]\n",
        "print()\n",
        "bow=[]\n",
        "for sen in sentence:\n",
        "  tokens = word_tokenize(sen.lower())\n",
        "  vector = []\n",
        "  for w in filtered_vocab:\n",
        "    vector.append(tokens.count(w))\n",
        "  bow.append(vector)\n",
        "\n",
        "\n",
        "print(\"Bag Of Words and Vectors for all Sentences are : \")\n",
        "print(filtered_vocab)\n",
        "for i in range(len(bow)):\n",
        "  print(\"SENTENCE-\",i+1,\":\",sentence[i])\n",
        "  print(bow[i])\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QNO 5. From the above paragraph print frequency of each word using NLTK?"
      ],
      "metadata": {
        "id": "NsP3IiXTcfc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "paragraph = \"\"\"The NLTK library is one of the oldest and most commonly used Python libraries \n",
        "for Natural Language Processing. NLTK supports stop word removal, \n",
        "and you can find the list of stop words in the corpus module. \n",
        "To remove stop words from a sentence, you can divide your text into words \n",
        "and then remove the word if it exits in the list of stop words provided by NLTK.\"\"\"\n",
        "  \n",
        "tokens = word_tokenize(paragraph)\n",
        "freq = FreqDist()\n",
        "\n",
        "for word in tokens: \n",
        "    freq[word.lower()]+= 1\n",
        "\n",
        "print(\"Frequency of each word in the given paragraph is : \")\n",
        "result = dict(freq)\n",
        "for i in result:\n",
        "  print(i, '=', result[i])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4el6g0sevBz",
        "outputId": "41d77b29-2109-4a24-adee-ad8cc169a1c9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of each word in the given paragraph is : \n",
            "the = 6\n",
            "nltk = 3\n",
            "library = 1\n",
            "is = 1\n",
            "one = 1\n",
            "of = 3\n",
            "oldest = 1\n",
            "and = 3\n",
            "most = 1\n",
            "commonly = 1\n",
            "used = 1\n",
            "python = 1\n",
            "libraries = 1\n",
            "for = 1\n",
            "natural = 1\n",
            "language = 1\n",
            "processing = 1\n",
            ". = 3\n",
            "supports = 1\n",
            "stop = 4\n",
            "word = 2\n",
            "removal = 1\n",
            ", = 2\n",
            "you = 2\n",
            "can = 2\n",
            "find = 1\n",
            "list = 2\n",
            "words = 4\n",
            "in = 2\n",
            "corpus = 1\n",
            "module = 1\n",
            "to = 1\n",
            "remove = 2\n",
            "from = 1\n",
            "a = 1\n",
            "sentence = 1\n",
            "divide = 1\n",
            "your = 1\n",
            "text = 1\n",
            "into = 1\n",
            "then = 1\n",
            "if = 1\n",
            "it = 1\n",
            "exits = 1\n",
            "provided = 1\n",
            "by = 1\n"
          ]
        }
      ]
    }
  ]
}